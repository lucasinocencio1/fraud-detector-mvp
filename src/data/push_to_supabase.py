import os
import time
import pandas as pd
from supabase import create_client, Client
from dotenv import load_dotenv
import requests

# ============================================================
# Script: push_to_supabase.py
# Sincroniza o schema do Supabase com o CSV e envia os dados.
# ============================================================

# Carregar variÃ¡veis de ambiente
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL") or "https://unoabldfhxojxlcvsalr.supabase.co"
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVub2FibGRmaHhvanhsY3ZzYWxyIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjA0NzU4MTYsImV4cCI6MjA3NjA1MTgxNn0.2Ihoh_iKVOQ7xg3FHRDIn7EJFdqOL2W0TFTqHZ1zH08"

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ============================================================
# FunÃ§Ã£o auxiliar: verifica e atualiza schema automaticamente
# ============================================================

def sync_schema_with_csv(table_name: str, df: pd.DataFrame):
    """Sincroniza as colunas do Supabase com o CSV local."""
    print(f"\nğŸ” Verificando schema da tabela '{table_name}'...")

    headers = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
    }

    response = requests.get(f"{SUPABASE_URL}/rest/v1/{table_name}?limit=1", headers=headers)
    if response.status_code != 200:
        print(f"âš ï¸ NÃ£o foi possÃ­vel acessar a tabela '{table_name}': {response.text}")
        return

    # Obter colunas atuais do Supabase
    remote_columns = response.json()[0].keys() if response.json() else []
    local_columns = df.columns

    # Detectar diferenÃ§as
    missing_cols = [col for col in local_columns if col not in remote_columns]
    extra_cols = [col for col in remote_columns if col not in local_columns]

    if missing_cols:
        print(f"â• Colunas ausentes no Supabase: {missing_cols}")
    if extra_cols:
        print(f"â– Colunas extras no Supabase (serÃ£o ignoradas no upload): {extra_cols}")
    if not missing_cols and not extra_cols:
        print("âœ… Schema estÃ¡ sincronizado com o CSV.")
        return

    # Adicionar colunas ausentes (via RPC ou SQL)
    for col in missing_cols:
        print(f"ğŸ› ï¸ Criando coluna '{col}' no Supabase...")
        try:
            sql = f'ALTER TABLE {table_name} ADD COLUMN "{col}" text;'
            rpc_resp = supabase.postgrest.rpc("exec_sql", {"sql": sql}).execute()
            print(f"   â†³ Resultado: {rpc_resp}")
        except Exception as e:
            if "already exists" in str(e):
                print(f"   â†³ Coluna '{col}' jÃ¡ existe, pulando...")
            else:
                print(f"   â†³ Erro ao criar coluna '{col}': {e}")
                # Tentar com tipo diferente se text falhar
                try:
                    sql = f'ALTER TABLE {table_name} ADD COLUMN "{col}" varchar;'
                    rpc_resp = supabase.postgrest.rpc("exec_sql", {"sql": sql}).execute()
                    print(f"   â†³ Resultado (varchar): {rpc_resp}")
                except Exception as e2:
                    print(f"   â†³ Erro tambÃ©m com varchar: {e2}")

    print("âœ… SincronizaÃ§Ã£o concluÃ­da.\n")


# ============================================================
# FunÃ§Ã£o para enviar DataFrame em batches
# ============================================================

def upload_table(df, table_name, conflict_field=None):
    batch_size = 5000
    print(f"\nğŸš€ Enviando {len(df)} registros para tabela '{table_name}'...")

    for i in range(0, len(df), batch_size):
        batch = df.iloc[i : i + batch_size].to_dict(orient="records")
        start = time.time()

        try:
            if conflict_field:
                res = supabase.table(table_name).upsert(batch, on_conflict=conflict_field).execute()
            else:
                res = supabase.table(table_name).insert(batch).execute()

            end = time.time()
            elapsed = round(end - start, 2)
            print(f"âœ… Batch {i // batch_size + 1} enviado ({len(batch)} registros) em {elapsed}s")

        except Exception as e:
            print(f"âŒ Erro ao enviar batch {i // batch_size + 1}: {e}")
            break


# ============================================================
# ExecuÃ§Ã£o principal
# ============================================================

if __name__ == "__main__":
    customers_path = "data/customers.csv"
    transactions_path = "data/transactions.csv"

    if not os.path.exists(customers_path):
        raise FileNotFoundError("âŒ Arquivo data/customers.csv nÃ£o encontrado.")
    if not os.path.exists(transactions_path):
        raise FileNotFoundError("âŒ Arquivo data/transactions.csv nÃ£o encontrado.")

    customers = pd.read_csv(customers_path)
    transactions = pd.read_csv(transactions_path)

    # Padronizar nomes
    customers.columns = [c.lower() for c in customers.columns]
    transactions.columns = [c.lower() for c in transactions.columns]

    # ============================================================
    # ğŸ”¹ AQUI ENTRA O CÃ“DIGO NOVO ğŸ”¹
    # ============================================================
    import uuid
    import numpy as np

    # Garantir que o customers.csv tenha UUID
    if "customer_id" not in customers.columns:
        print("ğŸ†• Adicionando coluna 'customer_id' com UUIDs Ãºnicos ao customers.csv...")
        customers["customer_id"] = [str(uuid.uuid4()) for _ in range(len(customers))]
    else:
        null_ids = customers["customer_id"].isna().sum() + (customers["customer_id"] == "").sum()
        if null_ids > 0:
            print(f"âš ï¸ {null_ids} customer_id nulos encontrados â€” regenerando UUIDs vÃ¡lidos.")
            customers["customer_id"] = [str(uuid.uuid4()) for _ in range(len(customers))]

    # Garantir que o transactions.csv tenha customer_id vÃ¡lido
    if "customer_id" not in transactions.columns:
        print("ğŸ”— Adicionando coluna 'customer_id' no transactions.csv vinculando clientes existentes...")
        transactions["customer_id"] = np.random.choice(customers["customer_id"], size=len(transactions))
    else:
        print("âœ… transactions.csv jÃ¡ contÃ©m coluna customer_id.")

    # Salvar versÃµes atualizadas
    customers.to_csv("data/customers.csv", index=False)
    transactions.to_csv("data/transactions.csv", index=False)
    print("ğŸ’¾ Arquivos atualizados: data/customers.csv e data/transactions.csv")

    # ============================================================
    # Sincronizar schema automaticamente
    # ============================================================
    sync_schema_with_csv("customers", customers)
    sync_schema_with_csv("transactions", transactions)

    # ============================================================
    # Limpeza e upload
    # ============================================================
    print("\nğŸ§¹ Limpando tabela 'customers' antes de subir novos registros...")
    try:
        supabase.table("customers").delete().neq("customer_id", "0").execute()
        print("âœ… Tabela 'customers' limpa com sucesso.")
    except Exception as e:
        print(f"âš ï¸ Aviso: limpeza direta falhou ({e}), prosseguindo mesmo assim.")

    # Garantir compatibilidade de tipos antes do upload
    if "is_night" in transactions.columns:
        # Converte 0/1 -> True/False para Supabase boolean
        transactions["is_night"] = transactions["is_night"].astype(bool)  
     
    # ============================================================
# Garantir correspondÃªncia 1:1 entre customers e transactions
# ============================================================

# Se nÃ£o existir a coluna customer_id nas transaÃ§Ãµes, cria e popula
if "customer_id" not in transactions.columns:
    print("\nğŸ”— Vinculando transaÃ§Ãµes a clientes (1:1)...")
    
    # Garantir que hÃ¡ pelo menos 1 cliente
    if "customer_id" not in customers.columns:
        raise ValueError("âŒ A tabela customers nÃ£o contÃ©m a coluna 'customer_id'. Verifique o arquivo customers.csv.")
    
    # Fazer correspondÃªncia aleatÃ³ria (1 cliente pode ter vÃ¡rias transaÃ§Ãµes)
    transactions["customer_id"] = customers["customer_id"].sample(
        n=len(transactions), replace=True, random_state=42
    ).values

    # Validar que os UUIDs sÃ£o vÃ¡lidos
    invalid_ids = transactions["customer_id"].isna().sum()
    if invalid_ids > 0:
        print(f"âš ï¸ {invalid_ids} customer_id nulos encontrados â€” preenchendo novamente.")
        transactions["customer_id"] = customers["customer_id"].sample(
            n=len(transactions), replace=True, random_state=42
        ).values

    print("âœ… customer_id adicionado e validado com sucesso.")        

    upload_table(customers, "customers", conflict_field="customer_id")
    upload_table(transactions, "transactions")

    print("\nğŸ¯ Upload completo para o Supabase com schema idÃªntico ao CSV!")
